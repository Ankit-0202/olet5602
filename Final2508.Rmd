---
title: Novel Factor Target Gene Prediction
subtitle: through Multi-Omics Datasets
author: "A. Kapoor, D. Langreiter, S. Udit, L. Richard"
date: "University of Sydney | OLET5602 | `r format(Sys.time(), '%B, %Y')`"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme:
      bg: "white"
      fg: "black"
      primary: "#050A30"
      base_font:
        google: Montserrat
      heading_font:
        google: Merriweather
  pdf_document: default
    extra_dependencies: ["geometry"]
header-includes:
  - \usepackage{listings}
  - \lstset{breaklines=true}
---

# Introduction

**Aim:**
The aim of the project is to classify genes as targets for novel transcription factors. The focus of the study in particularly on Sox2 and Nanog (of the OSN Factors::w Oct4, Sox2, Nanog) embryonic stem cell (ESC) differentiation. 

This is a particularly important, and ongoing, area of research as there is some evidence that OSN factors are regulators of stem cell maintenance.

**Background:**

**Background:**

Embryonic stem cells (ESCs) are remarkable for their ability to differentiate into a diverse array of cell types, a process fundamental to development and regenerative medicine. The regulation of ESC differentiation and cell fate decisions is intricately controlled by transcription factors, such as Sox2 and Nanog, which maintain pluripotency and guide the transition from undifferentiated states to specialized cell types. These transcription factors operate within complex transcriptional networks that influence gene expression through direct binding to DNA and interaction with other regulatory proteins ([Theunissen and Jaenisch, 2017](https://doi.org/10.1038/nrm.2017.15)).

Transcriptional regulation is crucial in defining cell identity and function during differentiation. Transcription factors can act as activators or repressors, modulating the expression of genes that drive cell lineage specification. Multiple studies have shown that Sox2 ([Masui et al., 2007](https://doi.org/10.1038/nature06015)) and Nanog ([Masui et al., 2007](https://doi.org/10.1038/nature06015)) are key regulators of stem cell maintenance and differentiation. Oct4, Sox2, and Nanog, collectively known as the 'OSN' factors, are well-established in their roles as regulators of stem cell maintenance ([Yeo and Ng, 2012](https://doi.org/10.1038/nrm3430)).

Recent advancements in omics technologies have enabled high-temporal-resolution profiling of genome-wide transcriptional and epigenetic events. The time-course multi-omic profiling of ESC differentiation provides a unique opportunity to reveal previously unknown aspects of stem cells during pluripotency progression ([Yang et al., 2019](https://doi.org/10.1016/j.cell.2019.02.035)). Multi-omics approaches, integrating genomics, transcriptomics, proteomics, and epigenomics, offer unprecedented insights into the regulatory networks governing pluripotency and differentiation.

In this study, we aim to predict novel target genes of Sox2 and Nanog by leveraging high-temporal-resolution multi-omics data from ESC differentiation experiments. By analyzing changes in gene expression, protein interactions, and epigenetic modifications, we seek to identify new substrates of these critical transcription factors. This approach will enhance our understanding of the transcriptional regulation of ESC differentiation and contribute to the development of more precise strategies for manipulating stem cell behavior in regenerative medicine.


**Dataset Overview:**

The data collects time-course differentiation of genes at various omics layers. The ones that the study will focus on are below.

For the Purposes of this study we refer to OSN Labels as genes that target for transcription factors: Sox2 and Nanog.

- **Transcriptome:** Time-course mRNA profiles during ESC differentiation.

- **Proteome:** Time-course protein expression profiles during ESC differentiation.

- **Epigenome:** Time-course ESC differentation epigonme profiles of 6 histone marks.

# EDA
## Load Required Libraries and Data

We start by loading the necessary R packages and the dataset `Final_Project_ESC.RData`, which contains the transcriptome, proteome, and epigenome data, along with a subset of known Sox2/Nanog target genes.

```{r setup, echo=FALSE}

knitr::opts_chunk$set(echo = TRUE)

htmltools::tags$style("
  pre {
    white-space: pre-wrap;      /* Wrap text within pre blocks */
    word-wrap: break-word;      /* Allows long words to be broken and wrapped */
    overflow-wrap: break-word;  /* Ensures wrapping occurs at the edge of the container */
  }
")
```

```{r}
# Load necessary packages and data
load("Final_Project_ESC.RData", verbose = TRUE)

suppressPackageStartupMessages({
    library(e1071)
    library(ggplot2)
    library(ROCR)
    library(calibrate)
    library(dplyr)
    library(tibble)
    library(reshape2)
    library(kernlab)
    library(caret)
    library(randomForest)
    library(adabag)
    library(gbm)
    library(xgboost)
    library(nnet)
    library(pROC)
    library(doParallel)
    library(calibrate)
})

set.seed(123)

```

## Describe and explore the Data set details

Before beginning data analysis it is important understand and investigate the data. The goal of this report is to predict to predict novel transcription factor target genes from multi-omics data. For each of our datasets, one can look at the structure of the data and perform PCA Analysis as means of identifying trends in the dataset.

Below is the temporal expression of the Triptome, Proteome, and Epigiome datasets at timepoints 0, 1, 6, 12, 24, 36, 48 hours.



### Transcriptome

```{r}
head(Transcriptome)
dim(Transcriptome)
colnames(Transcriptome)

# PCA analysis on the correlation matrix of the transcriptome data
cor.mat <- cor(Transcriptome)
pca.mat <- prcomp(cor.mat)

# Plot the PCA
grp <- rownames(pca.mat$x)
grp.col <- rainbow(nrow(pca.mat$x))
names(grp.col) <- rownames(pca.mat$x)

# Generate PCA plot
plot(pca.mat$x[,1], pca.mat$x[,2], col=grp.col[grp], pch=19, cex=2,
     xlab=paste0("PC1 (", round(summary(pca.mat)$importance[2,1]*100,1), "% variance)"),
     ylab=paste0("PC2 (", round(summary(pca.mat)$importance[2,2]*100,1), "% variance)"))

# Add sample labels to the plot
calibrate::textxy(pca.mat$x[,1], pca.mat$x[,2], labs=grp, cex=0.5)

```

### Proteome

```{r}
cor.proteome <- cor(Proteome)
pca.proteome <- prcomp(cor.proteome)
summary(pca.proteome)$importance

# Using the previous correlation matrix and PCA results
cor.proteome <- cor(Proteome)
pca.proteome <- prcomp(cor.proteome)

# Get group labels and colors
grp <- rownames(pca.proteome$x)  # Set groups according to your data
grp.col <- rainbow(nrow(pca.proteome$x))
names(grp.col) <- rownames(pca.proteome$x)

# Plot the PCA
plot(pca.proteome$x[,1], pca.proteome$x[,2], col=grp.col[grp], pch=19, cex=2,
     xlab=paste0("PC1 (", round(summary(pca.proteome)$importance[2,1]*100,1), "% variance)"),
     ylab=paste0("PC2 (", round(summary(pca.proteome)$importance[2,2]*100,1), "% variance)"))

```


### h3k4me3

```{r}
# PCA analysis on the correlation matrix of the H3K4me3 data
cor.h3k4me3 <- cor(H3K4me3)
pca.h3k4me3 <- prcomp(cor.h3k4me3)

# Get group labels and colors
grp <- rownames(pca.h3k4me3$x)
grp.col <- rainbow(nrow(pca.h3k4me3$x))
names(grp.col) <- rownames(pca.h3k4me3$x)

# Generate PCA plot for H3K4me3
plot(pca.h3k4me3$x[,1], pca.h3k4me3$x[,2], col=grp.col[grp], pch=19, cex=2,
     xlab=paste0("PC1 (", round(summary(pca.h3k4me3)$importance[2,1]*100,1), "% variance)"),
     ylab=paste0("PC2 (", round(summary(pca.h3k4me3)$importance[2,2]*100,1), "% variance)"))

# Add sample labels to the plot
calibrate::textxy(pca.h3k4me3$x[,1], pca.h3k4me3$x[,2], labs=grp, cex=0.5)

```

### H3K27me3

```{r}

# PCA analysis for H3K27me3 data
cor.h3k27me3 <- cor(H3K27me3)
pca.h3k27me3 <- prcomp(cor.h3k27me3)

# Update group labels and colors for H3K27me3
grp.h3k27me3 <- rownames(pca.h3k27me3$x)
grp.col.h3k27me3 <- rainbow(nrow(pca.h3k27me3$x))
names(grp.col.h3k27me3) <- rownames(pca.h3k27me3$x)

# Generate PCA plot for H3K27me3
plot(pca.h3k27me3$x[,1], pca.h3k27me3$x[,2], col=grp.col.h3k27me3[grp.h3k27me3], pch=19, cex=2,
     xlab=paste0("PC1 (", round(summary(pca.h3k27me3)$importance[2,1]*100,1), "% variance)"),
     ylab=paste0("PC2 (", round(summary(pca.h3k27me3)$importance[2,2]*100,1), "% variance)"))

# Correctly label the samples for H3K27me3
calibrate::textxy(pca.h3k27me3$x[,1], pca.h3k27me3$x[,2], labs=grp.h3k27me3, cex=0.5)

```

### H3K27ac

```{r}

# PCA analysis for H3K27ac data
cor.h3k27ac <- cor(H3K27ac)
pca.h3k27ac <- prcomp(cor.h3k27ac)

# Update group labels and colors for H3K27ac
grp.h3k27ac <- rownames(pca.h3k27ac$x)
grp.col.h3k27ac <- rainbow(nrow(pca.h3k27ac$x))
names(grp.col.h3k27ac) <- rownames(pca.h3k27ac$x)

# Generate PCA plot for H3K27ac
plot(pca.h3k27ac$x[,1], pca.h3k27ac$x[,2], col=grp.col.h3k27ac[grp.h3k27ac], pch=19, cex=2,
     xlab=paste0("PC1 (", round(summary(pca.h3k27ac)$importance[2,1]*100,1), "% variance)"),
     ylab=paste0("PC2 (", round(summary(pca.h3k27ac)$importance[2,2]*100,1), "% variance)"))

# Correctly label the samples for H3K27ac
calibrate::textxy(pca.h3k27ac$x[,1], pca.h3k27ac$x[,2], labs=grp.h3k27ac, cex=0.5)

```


### H3K4me1

```{r}
# PCA analysis for H3K4me1 data
cor.h3k4me1 <- cor(H3K4me1)
pca.h3k4me1 <- prcomp(cor.h3k4me1)

# Define the group labels and colors specifically for H3K4me1 data
grp.h3k4me1 <- rownames(pca.h3k4me1$x)
grp.col.h3k4me1 <- rainbow(nrow(pca.h3k4me1$x))
names(grp.col.h3k4me1) <- rownames(pca.h3k4me1$x)

# Generate PCA plot for H3K4me1
plot(pca.h3k4me1$x[,1], pca.h3k4me1$x[,2], col=grp.col.h3k4me1[grp.h3k4me1], pch=19, cex=2,
     xlab=paste0("PC1 (", round(summary(pca.h3k4me1)$importance[2,1]*100,1), "% variance)"),
     ylab=paste0("PC2 (", round(summary(pca.h3k4me1)$importance[2,2]*100,1), "% variance)"))

# Correctly label the samples for H3K4me1
calibrate::textxy(pca.h3k4me1$x[,1], pca.h3k4me1$x[,2], labs=grp.h3k4me1, cex=0.5)
```

### H3K9me2

```{r}
# PCA analysis for H3K9me2 data
cor.h3k9me2 <- cor(H3K9me2)
pca.h3k9me2 <- prcomp(cor.h3k9me2)

# Define the group labels and colors specifically for H3K9me2 data
grp.h3k9me2 <- rownames(pca.h3k9me2$x)
grp.col.h3k9me2 <- rainbow(nrow(pca.h3k9me2$x))
names(grp.col.h3k9me2) <- rownames(pca.h3k9me2$x)

# Generate PCA plot for H3K9me2
plot(pca.h3k9me2$x[,1], pca.h3k9me2$x[,2], col=grp.col.h3k9me2[grp.h3k9me2], pch=19, cex=2,
     xlab=paste0("PC1 (", round(summary(pca.h3k9me2)$importance[2,1]*100,1), "% variance)"),
     ylab=paste0("PC2 (", round(summary(pca.h3k9me2)$importance[2,2]*100,1), "% variance)"))

# Correctly label the samples for H3K9me2
calibrate::textxy(pca.h3k9me2$x[,1], pca.h3k9me2$x[,2], labs=grp.h3k9me2, cex=0.5)

```


### PolII

```{r}
# PCA analysis for PolII data
cor.polii <- cor(PolII)
pca.polii <- prcomp(cor.polii)

# Define the group labels and colors specifically for PolII data
grp.polii <- rownames(pca.polii$x)
grp.col.polii <- rainbow(nrow(pca.polii$x))
names(grp.col.polii) <- rownames(pca.polii$x)

# Generate PCA plot for PolII
plot(pca.polii$x[,1], pca.polii$x[,2], col=grp.col.polii[grp.polii], pch=19, cex=2,
     xlab=paste0("PC1 (", round(summary(pca.polii)$importance[2,1]*100,1), "% variance)"),
     ylab=paste0("PC2 (", round(summary(pca.polii)$importance[2,2]*100,1), "% variance)"))

# Correctly label the samples for PolII
calibrate::textxy(pca.polii$x[,1], pca.polii$x[,2], labs=grp.polii, cex=0.5)
```

# Classification

## Filter and Combine Datasets
In order to properly model and predicting novel transcription factors target genes we join the 8 datasets together and perform our calculations on the larger dataset.

To ensure consistency across datasets, we filter each dataset to include only the common genes present in all omics layers. We then combine these filtered datasets into a single data frame for further analysis.

```{r}
# Ensure all data has the same set of genes
genes <- intersect(rownames(Transcriptome), rownames(Proteome))
genes <- intersect(genes, rownames(H3K4me3))
genes <- intersect(genes, rownames(H3K27me3))
genes <- intersect(genes, rownames(H3K27ac))
genes <- intersect(genes, rownames(H3K4me1))
genes <- intersect(genes, rownames(H3K9me2))
genes <- intersect(genes, rownames(PolII))
```


```{r}
# Filter each dataset for the common genes
Transcriptome_filter <- Transcriptome[genes, ]
Proteome_filter <- Proteome[genes, ]
H3K4me3_filter <- H3K4me3[genes, ]
H3K27me3_filter <- H3K27me3[genes, ]
H3K27ac_filter <- H3K27ac[genes, ]
H3K4me1_filter <- H3K4me1[genes, ]
H3K9me2_filter <- H3K9me2[genes, ]
PolII_filter <- PolII[genes, ]

# Confirm that all datasets share the same gene
identical(rownames(Transcriptome_filter), rownames(H3K4me3_filter))
identical(rownames(Proteome_filter), rownames(H3K4me3_filter))
identical(rownames(H3K27ac_filter), rownames(H3K4me3_filter))
identical(rownames(H3K4me1_filter), rownames(H3K4me3_filter))
identical(rownames(H3K9me2_filter), rownames(H3K4me3_filter))
identical(rownames(PolII_filter), rownames(H3K4me3_filter))
```

```{r}
# Rename columns to avoid conflicts
colnames(Transcriptome_filter) <- paste("T_", colnames(Transcriptome_filter), sep = "")
colnames(Proteome_filter) <- paste("P_", colnames(Proteome_filter), sep = "")
colnames(H3K4me3_filter) <- paste("H3K4me3_", colnames(H3K4me3_filter), sep = "")
colnames(H3K27me3_filter) <- paste("H3K27me3_", colnames(H3K27me3_filter), sep = "")
colnames(H3K27ac_filter) <- paste("H3K27ac_", colnames(H3K27ac_filter), sep = "")
colnames(H3K4me1_filter) <- paste("H3K4me1_", colnames(H3K4me1_filter), sep = "")
colnames(H3K9me2_filter) <- paste("H3K9me2_", colnames(H3K9me2_filter), sep = "")
colnames(PolII_filter) <- paste("PolII_", colnames(PolII_filter), sep = "")

# Combine the datasets
combined_data <- cbind(
  Transcriptome_filter,
  Proteome_filter,
  H3K4me3_filter,
  H3K27me3_filter,
  H3K27ac_filter,
  H3K4me1_filter,
  H3K9me2_filter,
  PolII_filter
)

# Add the labels
label <- ifelse(genes %in% OSN_target_genes_subset, "OSN", "Other")
combined_data <- data.frame(combined_data)
combined_data$label <- factor(label)
```


```{r}
# Number of genes which are known to be targets for Sox2 and Nanog
length(OSN_target_genes_subset)
```


We have 100 known target genes for OSN, and as seen below the we have 95 genes that have been identified as novel Sox2/Nanog targets on our combined filtered dataset.

```{r}
# Check the initial label distribution
print(table(combined_data$label))
```

### Data Splitting and Balancing

The dataset is split into training (90%) and testing (10%) sets. The label column is reassigned to the test set to ensure that it is included correctly.

```{r}
# Split the dataset into training (90%) and testing (10%) sets
set.seed(123)
train_index <- createDataPartition(combined_data$label, p = 0.9, list = FALSE)

train_data <- combined_data[train_index, ]
test_data <- combined_data[-train_index, ]

# Reassign the label column to test_data
test_data$label <- combined_data[-train_index, "label"]

# Check the distribution of labels in the training and test sets
print("Training set label distribution:")
print(table(train_data$label))

print("Test set label distribution:")
print(table(test_data$label))

```

### Balancing the Training Data

To address the imbalance in the dataset, downsampling is employed to ensure both classes, `OSN` and `Other`, are represented equally. This technique enhances model accuracy and generalization by preventing bias towards the more frequent class.

```{r}
# Balance the training data using downsampling
set.seed(123)
downsampled_train_data <- downSample(x = train_data[, -ncol(train_data)],
                                     y = train_data$label,
                                     list = FALSE, yname = "label")

# Display the new balanced label distribution
print("Balanced training set label distribution:")
table(downsampled_train_data$label)

# Display dimensions of the balanced training data
dim(downsampled_train_data)
```

```{r}
# Final check of training dataset dimensions
print(dim(downsampled_train_data))

```

## Model Training
<<<<<<< HEAD
=======
We train two machine learning models, SVM (with the radial kernel) and Random Forest, using the balanced training dataset.
>>>>>>> 93663ae (Add theme and format)

Two models, SVM with a radial kernel and Random Forest, are trained using the balanced dataset to compare their performance under balanced class distribution conditions.

```{r}
# Train an SVM model on the downsampled training data with radial basis function kernel
svm_model <- svm(label ~ ., data = downsampled_train_data, kernel = "radial", probability = TRUE)

# Train a Random Forest model
rf_model <- randomForest(label ~ ., data = downsampled_train_data, ntree = 1000)

# Extract and plot feature importance
importance <- importance(rf_model)
ord <- order(importance, decreasing = TRUE)
barplot(importance[ord], main = "Feature Importance in Random Forest", col = 'blue')

```

We can then perform corss-validation and finetune the hyperparameters, being careful not to overfit the model.

```{r}

# Define tuning grid and control setup
tuning_grid <- expand.grid(mtry = seq(2, 5, by = 1))
control <- trainControl(method = "cv", number = 5)

# Tuning the model
set.seed(123)
tuned_rf_model <- train(label ~ ., data = downsampled_train_data,
                        method = "rf", trControl = control, tuneGrid = tuning_grid)

# Plotting tuning results
plot(tuned_rf_model$finalModel, main="Random Forest Tuning")

```

Before conducting proper evaluation later on this report, we can briefly evaluate the performance of this model.

```{r}

rf_predictions <- predict(tuned_rf_model, newdata = test_data, type = "prob")
roc_curve <- roc(response = test_data$label, predictor = as.numeric(rf_predictions[,2]))

# Plot ROC curve
plot(roc_curve, main = "ROC Curve for Random Forest")

```

Upon viewing the performance of the model above, it was clear that more could be done to optimise the model. As a result, the team decided to experiment with other models to evaluate the effectiveness of variations of standard random forest models.

### Bagging w/ Bagged Trees

Bagging improves stability and accuracy by reducing variance and avoiding overfitting.

```{r}

bagged_trees <- train(
  label ~ .,
  data = downsampled_train_data,
  method = "treebag",
  trControl = trainControl(method = "cv", number = 5),
  tuneLength = 5
)
print(bagged_trees)

```

### Gradient Boosting with Hyperparameter Tuning Using xgboost (using parallel processing)

For hyperparameters tuning we define the grid of paramaters below and train an XGB Model on the downsampled data. We use cross validation as means to providing a reliable model that is not overfit, and allow the model to be trained in parallel.

```{r}

# Set up a tuning grid for xgboost
tune_grid_xgb <- expand.grid(
  nrounds = c(100, 200),
  max_depth = c(3, 6),
  eta = c(0.1, 0.3),
  gamma = c(0, 0.1),
  colsample_bytree = c(0.5, 1),
  min_child_weight = c(1, 10),
  subsample = c(0.5, 1)
)

# Enable parallel processing
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)

# Define control function for training
train_control <- trainControl(
  method = "cv",
  number = 3,
  savePredictions = "final",
  verboseIter = TRUE,
  allowParallel = TRUE
)

# Train xgboost model with tuning
library(caret)
xgb_model_tuned <- train(
  label ~ .,
  data = downsampled_train_data,
  method = "xgbTree",
  trControl = train_control,
  tuneGrid = tune_grid_xgb,
  metric = "Accuracy"
)
print(xgb_model_tuned$bestTune)

# Plot model performance
plot(xgb_model_tuned)

```

## Extended Model Training - Neural Networks

To extend our understanding of the dataset and compare our Random Forest models against other classifiers, the team decided to evaluate the use of a neural network on the same training data.

### Preparing Data and Feature Scaling

Proper data scaling is necessary for the performance of a Neural Network due to the sensitivity of the implementation of the algorithm in R to the scale of input variables.

```{r}

scaled_data <- scale(downsampled_train_data[, -ncol(downsampled_train_data)])
scaled_train_data <- data.frame(scaled_data, label = downsampled_train_data$label)

```

Then, as was done with the Random Forest model, the team trained the Neural Network with varying architectural parameters and visualizing the tuning process to identify the best model settings.

```{r}

# Setup for Neural Network training
control_nn <- trainControl(method = "cv", number = 5, savePredictions = "final")
grid_nn <- expand.grid(.size = c(5, 10), .decay = c(0.1, 0.01))

# Train the Neural Network
set.seed(123)
nn_model <- train(label ~ ., data = scaled_train_data, method = "nnet", trControl = control_nn, tuneGrid = grid_nn, trace = FALSE)

```

We can then plot the model's performance.

```{r}

plot(nn_model)

```

## Model Evaluation
Predictions and Confusion Matrix
We use the trained models to make predictions on the test set and evaluate their performance using confusion matrices.

```{r}
# Ensure column names in the test set match the training data
colnames(test_data) <- colnames(downsampled_train_data)[1:(ncol(downsampled_train_data) - 1)]  # Exclude the label column

# Check if the label column is present and correctly populated
if ("label" %in% colnames(combined_data) && length(combined_data[-train_index, "label"]) == nrow(test_data)) {
    # Assign the label to test_data
    test_data$label <- combined_data[-train_index, "label"]
} else {
    stop("The label vector is empty or has a different length than expected. Check the data preparation steps.")
}

# Ensure the label is a factor with the correct levels
test_data$label <- factor(test_data$label, levels = c("OSN", "Other"))

# SVM Predictions on the test set
svm_test_pred <- predict(svm_model, newdata = test_data[, -ncol(test_data)], probability = TRUE)
svm_test_prob <- attr(svm_test_pred, "probabilities")[, "OSN"]

# Random Forest Predictions on the test set
rf_test_pred <- predict(rf_model, newdata = test_data[, -ncol(test_data)], type = "prob")[, "OSN"]

# SVM Confusion Matrix on the test set
svm_test_conf_matrix <- table(Predicted = ifelse(svm_test_prob > 0.5, "OSN", "Other"), Actual = test_data$label)
print("SVM Test Confusion Matrix:")
print(svm_test_conf_matrix)

# Random Forest Confusion Matrix on the test set
rf_test_conf_matrix <- table(Predicted = ifelse(rf_test_pred > 0.5, "OSN", "Other"), Actual = test_data$label)
print("Random Forest Test Confusion Matrix:")
print(rf_test_conf_matrix)

```

## ROC Curve and AUC
To further assess model performance, we plot the ROC curves and calculate the AUC for both the SVM and Random Forest models.

```{r}
# Evaluate the models on the test set (AUC and ROC)
svm_test_roc <- roc(test_data$label, svm_test_prob)
plot(svm_test_roc, main = "SVM ROC Curve")
print(paste("Final SVM Test AUC:", auc(svm_test_roc)))

rf_test_roc <- roc(test_data$label, rf_test_pred)
plot(rf_test_roc, main = "Random Forest ROC Curve")
print(paste("Final Random Forest Test AUC:", auc(rf_test_roc)))

```


## Future Directions:

**Model Improvement:**
Further hyperparameter tuning using automated methods like grid search or random search could refine the model's accuracy. Additionally, exploring ensemble methods that combine predictions from several models might yield better results.

**Data Expansion:**
Incorporating additional omics layers, such as metabolomics or additional transcription factor binding profiles, could help to improve the model's predictive power and generalisability.

**Integration with Clinical Data:**
Linking omics profiles with clinical outcomes could also be explored to improve the translational impact of the research.
